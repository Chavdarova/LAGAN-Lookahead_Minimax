{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQ1cgc7Y_0bH"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gGThtfYc_lKU"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import torchvision.utils as vision_utils\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.distributions import bernoulli\n",
    "from scipy import linalg\n",
    "import torchvision.datasets as _datasets\n",
    "import torchvision.transforms as _transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KltKnDnTw8sM"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7rmPgOn_4Yk"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "suSPfCc-_0vc"
   },
   "outputs": [],
   "source": [
    "_NOISE_DIM = 128\n",
    "_H_FILTERS = 64\n",
    "\n",
    "\n",
    "class DiscriminatorCNN28(nn.Module):\n",
    "\n",
    "    def __init__(self, img_channels=1, h_filters=_H_FILTERS,\n",
    "                 spectral_norm=False, img_size=None, n_outputs=1):\n",
    "        if any(not isinstance(_arg, int) for _arg in [img_channels, h_filters, n_outputs]):\n",
    "            raise TypeError(\"Unsupported operand type. Expected integer.\")\n",
    "        if not isinstance(spectral_norm, bool):\n",
    "            raise TypeError(f\"Unsupported operand type: {type(spectral_norm)}. \"\n",
    "                            \"Expected bool.\")\n",
    "        if min([img_channels, h_filters, n_outputs]) <= 0:\n",
    "            raise ValueError(\"Expected nonzero positive input arguments for: the \"\n",
    "                             \"number of output channels, the dimension of the noise \"\n",
    "                             \"vector, as well as the depth of the convolution kernels.\")\n",
    "        super(DiscriminatorCNN28, self).__init__()\n",
    "        # _conv = nn.utils.spectral_norm(nn.Conv2d) if spectral_norm else nn.Conv2d\n",
    "        _apply_sn = lambda x: nn.utils.spectral_norm(x) if spectral_norm else x\n",
    "        self.img_channels = img_channels\n",
    "        self.img_size = img_size\n",
    "        self.n_outputs = n_outputs\n",
    "        self.main = nn.Sequential(\n",
    "            _apply_sn(nn.Conv2d(img_channels, h_filters, 4, 2, 1, bias=False)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            _apply_sn(nn.Conv2d(h_filters, h_filters * 2, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(h_filters * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            _apply_sn(nn.Conv2d(h_filters * 2, h_filters * 4, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(h_filters * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            _apply_sn(nn.Conv2d(h_filters * 4, self.n_outputs, 3, 1, 0, bias=False))        \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.img_channels is not None and self.img_size is not None:\n",
    "            if numpy.prod(list(x.size())) % (self.img_size ** 2 * self.img_channels) != 0:\n",
    "                raise ValueError(f\"Size mismatch. Input size: {numpy.prod(list(x.size()))}. \"\n",
    "                                 f\"Expected input divisible by: {self.noise_dim}\")\n",
    "            x = x.view(-1, self.img_channels, self.img_size, self.img_size)\n",
    "        x = self.main(x)\n",
    "        return x.view(-1, self.n_outputs)\n",
    "\n",
    "    def load(self, model):\n",
    "      self.load_state_dict(model.state_dict())\n",
    "\n",
    "\n",
    "class GeneratorCNN28(nn.Module):\n",
    "\n",
    "    def __init__(self, img_channels=1, noise_dim=_NOISE_DIM, h_filters=_H_FILTERS, out_tanh=False):\n",
    "        if any(not isinstance(_arg, int) for _arg in [img_channels, noise_dim, h_filters]):\n",
    "            raise TypeError(\"Unsupported operand type. Expected integer.\")\n",
    "        if min([img_channels, noise_dim, h_filters]) <= 0:\n",
    "            raise ValueError(\"Expected strictly positive input arguments for the \"\n",
    "                             \"number of output channels, the dimension of the noise \"\n",
    "                             \"vector, as well as the depth of the convolution kernels.\")\n",
    "        super(GeneratorCNN28, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(noise_dim, h_filters * 8, 3, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(_H_FILTERS * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(h_filters * 8, h_filters * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(_H_FILTERS * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(h_filters * 4, h_filters * 2, 4, 2, 0, bias=False),\n",
    "            nn.BatchNorm2d(_H_FILTERS * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(h_filters * 2, img_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh() if out_tanh else nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if numpy.prod(list(x.size())) % self.noise_dim != 0:\n",
    "            raise ValueError(f\"Size mismatch. Input size: {numpy.prod(list(x.size()))}. \"\n",
    "                             f\"Expected input divisible by: {self.noise_dim}\")\n",
    "        x = x.view(-1, self.noise_dim, 1, 1)\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "\n",
    "    def load(self, model):\n",
    "      self.load_state_dict(model.state_dict())\n",
    "\n",
    "\n",
    "class MLP_mnist(nn.Module):\n",
    "  def __init__(self, input_dims, n_hiddens, n_class):\n",
    "    super(MLP_mnist, self).__init__()\n",
    "    assert isinstance(input_dims, int), 'Expected int for input_dims'\n",
    "    self.input_dims = input_dims\n",
    "    current_dims = input_dims\n",
    "    layers = OrderedDict()\n",
    "\n",
    "    if isinstance(n_hiddens, int):\n",
    "      n_hiddens = [n_hiddens]\n",
    "    else:\n",
    "      n_hiddens = list(n_hiddens)\n",
    "    for i, n_hidden in enumerate(n_hiddens):\n",
    "      layers['fc{}'.format(i+1)] = nn.Linear(current_dims, n_hidden)\n",
    "      layers['relu{}'.format(i+1)] = nn.ReLU()\n",
    "      layers['drop{}'.format(i+1)] = nn.Dropout(0.2)\n",
    "      current_dims = n_hidden\n",
    "    layers['out'] = nn.Linear(current_dims, n_class)\n",
    "    self.layers = layers\n",
    "    self.model= nn.Sequential(layers)\n",
    "    #print(self.model)\n",
    "\n",
    "  def forward(self, input):\n",
    "    input = input.view(input.size(0), -1)\n",
    "    assert input.size(1) == self.input_dims\n",
    "    return self.model.forward(input)\n",
    "\n",
    "  def get_logits_and_fc2_outputs(self, x):\n",
    "    x = x.view(x.size(0), -1)\n",
    "    assert x.size(1) == self.input_dims\n",
    "    fc2_out = None\n",
    "    for l in self.model:\n",
    "      x = l(x)\n",
    "      if l == self.layers[\"fc2\"]:\n",
    "        fc2_out = x\n",
    "    return x, fc2_out\n",
    "\n",
    "\n",
    "def pretrained_mnist_model(input_dims=784, n_hiddens=[256, 256], n_class=10, \n",
    "                           pretrained=None):\n",
    "    model = MLP_mnist(input_dims, n_hiddens, n_class)\n",
    "    if pretrained is not None:\n",
    "        if os.path.exists(pretrained):\n",
    "            print('Loading trained model from %s' % pretrained)\n",
    "            state_dict = torch.load(pretrained,\n",
    "                    map_location='cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "            if 'parallel' in pretrained:\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in state_dict.items():\n",
    "                    name = k[7:]  # remove `module.`\n",
    "                    new_state_dict[name] = v\n",
    "                state_dict = new_state_dict\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Could not find pretrained model: {pretrained}.\")\n",
    "        model.load_state_dict(state_dict)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDYOE4e8DRAY"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylrrJcSnDQOP"
   },
   "outputs": [],
   "source": [
    "class Binarize(object):\n",
    "  def __init__(self, threshold=0.3):\n",
    "    self.threshold = threshold\n",
    "      \n",
    "  def __call__(self, t):\n",
    "    t = (t > self.threshold).float()\n",
    "    return t\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return self.__class__.__name__ + '(th={0})'.format(self.threshold)\n",
    "\n",
    "\n",
    "class Smooth(object):\n",
    "  def __init__(self, smooth=0.1):\n",
    "    self.smooth = smooth\n",
    "      \n",
    "  def __call__(self, t):\n",
    "    t[t == 1.] = 1 - self.smooth\n",
    "    t[t == 0.] = 0 + self.smooth\n",
    "    return t\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return self.__class__.__name__ + '(smooth={0})'.format(self.smooth)\n",
    "\n",
    "\n",
    "def load_mnist(_data_root='datasets', binarized=False, bin_th=0.3, smooth=None):\n",
    "    trans = [_transforms.ToTensor()]\n",
    "    if binarized:\n",
    "      binarizor = Binarize(bin_th)\n",
    "      trans.append(binarizor)\n",
    "    if smooth is not None:\n",
    "      smoother = Smooth(smooth)\n",
    "      trans.append(smoother)\n",
    "    trans = _transforms.Compose(trans)\n",
    "    _data = _datasets.MNIST(_data_root, train=True, download=True,\n",
    "                            transform=trans)\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y-YEPGTyKwYZ"
   },
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8yUpASSiu6cQ"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5, k_min=3, k_max=1000):\n",
    "        print(\"Using lookahead.\")\n",
    "        self.optimizer = optimizer\n",
    "        self.resample_k = (k <= 0)\n",
    "        self.k_min = k_min\n",
    "        self.k_max = k_max\n",
    "        self.k = k if k > 0 else random.randint(k_min, k_max)  # endpoints included\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "\n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "        if self.resample_k:\n",
    "            self.k = random.randint(self.k_min, self.k_max)\n",
    "\n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        return loss\n",
    "    \n",
    "    def increment_counter(self):\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] += 1\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XCCAWH7lKwkv"
   },
   "outputs": [],
   "source": [
    "def get_disciminator_loss(D, x_real, x_gen, lbl_real, lbl_fake):\n",
    "  \"\"\"\"\"\"\n",
    "  D_x = D(x_real)\n",
    "  D_G_z = D(x_gen)\n",
    "  lossD_real = torch.binary_cross_entropy_with_logits(D_x, lbl_real).mean()\n",
    "  lossD_fake = torch.binary_cross_entropy_with_logits(D_G_z, lbl_fake).mean()\n",
    "  lossD = lossD_real + lossD_fake\n",
    "  return lossD\n",
    "\n",
    "\n",
    "def get_generator_loss(G, D, z, lbl_real):\n",
    "  \"\"\"\"\"\"\n",
    "  D_G_z = D(G(z))\n",
    "  lossG = torch.binary_cross_entropy_with_logits(D_G_z, lbl_real).mean()\n",
    "  return lossG\n",
    "\n",
    "\n",
    "def get_sampler(dataset, batch_size, shuffle=True, num_workers=1, drop_last=True):\n",
    "  dataloader = DataLoader(dataset, batch_size, shuffle=shuffle, \n",
    "                          num_workers=num_workers, drop_last=drop_last)\n",
    "  dataloader_iterator = iter(dataloader)\n",
    "  def sampler():\n",
    "    nonlocal dataloader_iterator\n",
    "    try:\n",
    "        data = next(dataloader_iterator) \n",
    "    except StopIteration:\n",
    "        dataloader_iterator = iter(dataloader)\n",
    "        data = next(dataloader_iterator) \n",
    "    return data\n",
    "  return sampler\n",
    "\n",
    "\n",
    "def update_avg_gen(G, G_avg, n_gen_update):\n",
    "    \"\"\" Updates the uniform average generator. \"\"\"\n",
    "    l_param = list(G.parameters())\n",
    "    l_avg_param = list(G_avg.parameters())\n",
    "    if len(l_param) != len(l_avg_param):\n",
    "        raise ValueError(\"Got different lengths: {}, {}\".format(len(l_param), len(l_avg_param)))\n",
    "\n",
    "    for i in range(len(l_param)):\n",
    "        with torch.no_grad():\n",
    "            l_avg_param[i].data.copy_(l_avg_param[i].data.mul(n_gen_update).div(n_gen_update + 1.).add(\n",
    "                                      l_param[i].data.div(n_gen_update + 1.)))\n",
    "\n",
    "def update_ema_gen(G, G_ema, beta_ema=0.9999):\n",
    "    \"\"\" Updates the exponential moving average generator. \"\"\"\n",
    "    l_param = list(G.parameters())\n",
    "    l_ema_param = list(G_ema.parameters())\n",
    "    if len(l_param) != len(l_ema_param):\n",
    "        raise ValueError(\"Got different lengths: {}, {}\".format(len(l_param), len(l_ema_param)))\n",
    "\n",
    "    for i in range(len(l_param)):\n",
    "        with torch.no_grad():\n",
    "            l_ema_param[i].data.copy_(l_ema_param[i].data.mul(beta_ema).add(\n",
    "                l_param[i].data.mul(1-beta_ema)))\n",
    "\n",
    "\n",
    "def train(G, D, dataset, iterations, batch_size=32, lrD=0.01, lrG=0.01, \n",
    "          beta1=0.99, eval_every=100, n_workers=5, device=torch.device('cpu'), \n",
    "          grad_max_norm=1, plot_func=lambda a,b,c,d: None, extragrad=False, \n",
    "          lookahead=False, lookahead_k=5, eval_avg=False, out_dir=None):\n",
    "  \n",
    "  sampler = get_sampler(dataset, batch_size, shuffle=True, \n",
    "                        num_workers=n_workers, drop_last=True)\n",
    "\n",
    "  if extragrad:\n",
    "    D_extra = copy.deepcopy(D)\n",
    "    G_extra = copy.deepcopy(G)\n",
    "  else:\n",
    "    D_extra = D\n",
    "    G_extra = G\n",
    "\n",
    "  # Optimizers\n",
    "  optimizerD = torch.optim.Adam(D.parameters(), lr=lrD, betas=(beta1, 0.999))\n",
    "  optimizerG = torch.optim.Adam(G.parameters(), lr=lrG, betas=(beta1, 0.999))\n",
    "  if lookahead:\n",
    "    optimizerD = Lookahead(optimizerD, k=lookahead_k)\n",
    "    optimizerG = Lookahead(optimizerG, k=lookahead_k)\n",
    "\n",
    "  optimizerD_extra = torch.optim.Adam(D_extra.parameters(), lr=lrD, betas=(beta1, 0.999))\n",
    "  optimizerG_extra = torch.optim.Adam(G_extra.parameters(), lr=lrG, betas=(beta1, 0.999))\n",
    "\n",
    "  # LBLs\n",
    "  lbl_real = torch.ones( batch_size, 1, device=device)\n",
    "  lbl_fake = torch.zeros(batch_size, 1, device=device)\n",
    "\n",
    "  fixed_noise = torch.randn(100, G.noise_dim, device=device)\n",
    "\n",
    "  G.to(device)\n",
    "  D.to(device)\n",
    "\n",
    "  G_extra.to(device)\n",
    "  D_extra.to(device)\n",
    "\n",
    "  G_avg, G_ema = None, None\n",
    "  if eval_avg:\n",
    "    G_avg = copy.deepcopy(G)\n",
    "    G_ema = copy.deepcopy(G)\n",
    "\n",
    "  start_time = time.perf_counter()\n",
    "\n",
    "  for i in range(iterations):\n",
    "\n",
    "    # STEP 1: get G_{t+1} (G_extra)\n",
    "    if extragrad:\n",
    "      optimizerG_extra.zero_grad()\n",
    "      z = torch.randn(batch_size, G_extra.noise_dim, device=device)\n",
    "      lossG = get_generator_loss(G_extra, D, z, lbl_real)\n",
    "      lossG.backward()\n",
    "      optimizerG_extra.step()\n",
    "\n",
    "    # STEP 3: Get D_{t+1} (D_extra)\n",
    "    if extragrad:\n",
    "      optimizerD_extra.zero_grad()\n",
    "      x_real, _ = sampler()\n",
    "      x_real = x_real.to(device)\n",
    "      z = torch.randn(batch_size, G.noise_dim, device=device)\n",
    "      with torch.no_grad():\n",
    "        x_gen = G(z)\n",
    "      lossD = get_disciminator_loss(D_extra, x_real, x_gen, lbl_real, lbl_fake)\n",
    "      lossD.backward()\n",
    "      optimizerD_extra.step()\n",
    "\n",
    "    # STEP 2: D optimization step using G_extra\n",
    "    x_real, _ = sampler()\n",
    "    x_real = x_real.to(device)\n",
    "    z = torch.randn(batch_size, G.noise_dim, device=device)\n",
    "    with torch.no_grad():\n",
    "      x_gen = G_extra(z) # using G_{t+1}\n",
    "    optimizerD.zero_grad()\n",
    "    lossD = get_disciminator_loss(D, x_real, x_gen, lbl_real, lbl_fake)\n",
    "    lossD.backward()\n",
    "    if grad_max_norm is not None:\n",
    "      nn.utils.clip_grad_norm_(D.parameters(), grad_max_norm)\n",
    "    optimizerD.step()\n",
    "\n",
    "    # STEP 4: G optimization step using D_extra\n",
    "    z = torch.randn(batch_size, G.noise_dim, device=device)\n",
    "    optimizerG.zero_grad()\n",
    "    lossG = get_generator_loss(G, D_extra, z, lbl_real) # we use the unrolled D\n",
    "    lossG.backward()\n",
    "    if grad_max_norm is not None:\n",
    "      nn.utils.clip_grad_norm_(G.parameters(), grad_max_norm)\n",
    "    optimizerG.step()\n",
    "\n",
    "    if extragrad:\n",
    "      G_extra.load_state_dict(G.state_dict())\n",
    "      D_extra.load_state_dict(D.state_dict())\n",
    "\n",
    "    if eval_avg:\n",
    "      update_avg_gen(G, G_avg, i)\n",
    "      update_ema_gen(G, G_ema, beta_ema=0.9999)\n",
    "\n",
    "    if lookahead and (i+1) % lookahead_k == 0: \n",
    "      optimizerG.update_lookahead()\n",
    "      optimizerD.update_lookahead()\n",
    "\n",
    "    if i % 20000 == 0:\n",
    "      save_models(G, D, optimizerG, optimizerD, out_dir, suffix=f\"{i}\")\n",
    "\n",
    "    # Just plotting things\n",
    "    if i % eval_every == 0 or i == iterations-1:\n",
    "      if out_dir is not None:\n",
    "        save_models(G, D, optimizerG, optimizerD, out_dir, suffix=\"last\")\n",
    "      with torch.no_grad():\n",
    "        probas = torch.sigmoid(D(G(fixed_noise)))\n",
    "        mean_proba = probas.mean().cpu().item()\n",
    "        std_proba = probas.std().cpu().item()\n",
    "        samples = G(fixed_noise)\n",
    "      print(f\"Iter {i}: Mean proba from D(G(z)): {mean_proba:.4f} +/- {std_proba:.4f}\")\n",
    "      plot_func(samples.detach().cpu(), time_tick=time.perf_counter() - start_time, D=D, G=G, iteration=i, G_avg=G_avg, G_ema=G_ema)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JEMA5J179bCN"
   },
   "source": [
    "# Display & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tBw8aDhlAMx"
   },
   "outputs": [],
   "source": [
    "def compute_mu_sigma_pretrained_model(dataset, pretrained_clf):\n",
    "  dataloader = DataLoader(dataset, batch_size=512, num_workers=2, drop_last=True)\n",
    "  cuda = next(pretrained_clf.parameters()).is_cuda\n",
    "  all_fc2_out = []\n",
    "  pretrained_clf.eval()\n",
    "  for batch, _ in dataloader:\n",
    "    with torch.no_grad():\n",
    "      if cuda:\n",
    "        batch = batch.cuda()\n",
    "      _, fc2_out = pretrained_clf.get_logits_and_fc2_outputs(batch)\n",
    "    all_fc2_out.append(fc2_out.cpu())\n",
    "  all_fc2_out = torch.cat(all_fc2_out, dim=0).numpy()\n",
    "  mu_real = np.mean(all_fc2_out, axis=0)\n",
    "  sigma_real = np.cov(all_fc2_out, rowvar=False)\n",
    "  return mu_real, sigma_real\n",
    "\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\"\"\"\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n",
    "    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    # product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n",
    "        print(msg)\n",
    "        # warnings.warn(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    # numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(\"Imaginary component {}\".format(m))\n",
    "        covmean = covmean.real\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "\n",
    "\n",
    "def _calculate_metrics(pretrained_clf, G, dataset_length, mu_real, sigma_real, \n",
    "                       n_classes=10, batch_size=1024):\n",
    "    cuda = next(pretrained_clf.parameters()).is_cuda\n",
    "    if cuda:\n",
    "      device = torch.device('cuda')\n",
    "    else:\n",
    "      device = torch.device('cpu')\n",
    "    # Using pretrained clf to get predictions over fake data\n",
    "    inception_predictions, all_fc2_out, class_probas = [], [], []\n",
    "    dataloader = DataLoader(list(range(dataset_length)), batch_size, num_workers=2, drop_last=True)\n",
    "    pretrained_clf.eval()\n",
    "    for batch in dataloader:\n",
    "      with torch.no_grad():\n",
    "        noise = torch.randn(batch_size, G.noise_dim, device=device)\n",
    "        probas, fc2_out = pretrained_clf.get_logits_and_fc2_outputs(G(noise).view(batch_size, -1))\n",
    "      all_fc2_out.append(fc2_out.cpu())\n",
    "      class_probas.append(probas.cpu())\n",
    "    all_fc2_out = torch.cat(all_fc2_out, dim=0).numpy()\n",
    "    class_probas = torch.cat(class_probas, dim=0)\n",
    "    inception_predictions = torch.softmax(class_probas, dim=1).numpy()\n",
    "    class_probas = class_probas.numpy()\n",
    "    pred_prob = np.maximum(class_probas, 1e-20 * np.ones_like(class_probas))\n",
    "\n",
    "    y_vec = 1e-20 * np.ones((len(pred_prob), n_classes), dtype=np.float)  # pred label distr\n",
    "    gnd_vec = 0.1 * np.ones((1, n_classes), dtype=np.float)  # gnd label distr, uniform over classes\n",
    "\n",
    "    for i, label in enumerate(pred_prob):\n",
    "        y_vec[i, np.argmax(pred_prob[i])] = 1.0\n",
    "    y_vec = np.sum(y_vec, axis=0, keepdims=True)\n",
    "    y_vec = y_vec / np.sum(y_vec)\n",
    "\n",
    "    label_entropy = np.sum(-y_vec * np.log(y_vec)).tolist()\n",
    "    label_tv = np.true_divide(np.sum(np.abs(y_vec - gnd_vec)), 2).tolist()\n",
    "    label_l2 = np.sum((y_vec - gnd_vec) ** 2).tolist()\n",
    "\n",
    "    # --- is ----\n",
    "    inception_scores = []\n",
    "    for i in range(n_classes):\n",
    "        part = inception_predictions[(i * inception_predictions.shape[0]\n",
    "                                      // n_classes):((i + 1) * inception_predictions.shape[0]\n",
    "                                                     // n_classes), :]\n",
    "        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
    "        kl = np.mean(np.sum(kl, 1))\n",
    "        inception_scores.append(np.exp(kl))\n",
    "\n",
    "    mu = np.mean(all_fc2_out, axis=0)\n",
    "    sigma = np.cov(all_fc2_out, rowvar=False)\n",
    "    _fid = calculate_frechet_distance(mu, sigma, mu_real, sigma_real)\n",
    "\n",
    "    return (label_entropy, label_tv, label_l2,\n",
    "            float(np.mean(inception_scores)),\n",
    "            float(np.std(inception_scores)),\n",
    "            _fid)\n",
    "\n",
    "\n",
    "def get_metrics(pretrained_clf, dataset_length, mu_real, sigma_real, G):\n",
    "    \"\"\"Calculates entropy, TV, L2, and inception scores.\"\"\"\n",
    "    e, tv, l2, is_m, is_std, fid = _calculate_metrics(pretrained_clf,\n",
    "                                                      G,\n",
    "                                                      dataset_length,\n",
    "                                                      mu_real,\n",
    "                                                      sigma_real)\n",
    "    m_result = {\n",
    "        'entropy': e,\n",
    "        'TV': tv,\n",
    "        'L2': l2,\n",
    "        'inception_mean': is_m,\n",
    "        'inception_std': is_std,\n",
    "        'fid': fid\n",
    "    }\n",
    "    return m_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9T2JfCAd5SXA"
   },
   "outputs": [],
   "source": [
    "def save_models(G, D, opt_G, opt_D, out_dir, suffix):\n",
    "  torch.save(G.state_dict(), os.path.join(out_dir, f\"gen_{suffix}.pth\"))\n",
    "  torch.save(D.state_dict(), os.path.join(out_dir, f\"disc_{suffix}.pth\"))\n",
    "  torch.save(opt_G.state_dict(), os.path.join(out_dir, f\"gen_optim_{suffix}.pth\"))\n",
    "  torch.save(opt_D.state_dict(), os.path.join(out_dir, f\"disc_optim_{suffix}.pth\"))\n",
    "\n",
    "\n",
    "def get_plot_func(out_dir, img_size, num_samples_eval=10000, save_curves=None):\n",
    "  dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "  #shutil.rmtree(out_dir, ignore_errors=True)\n",
    "  #if not os.path.exists(out_dir):\n",
    "  #  os.makedirs(out_dir)\n",
    "  pretrained_clf = pretrained_mnist_model(pretrained='./drive/My Drive/Data/models/mnist.pth')\n",
    "  mu_real, sigma_real = compute_mu_sigma_pretrained_model(dataset, pretrained_clf)\n",
    "  inception_means, inception_stds, inception_means_ema, inception_means_avg, fids, fids_ema, fids_avg = [], [], [], [], [], [], []\n",
    "  iterations, times = [], []\n",
    "  def plot_func(samples, iteration, time_tick, G=None, D=None, G_avg=None, G_ema=None):\n",
    "    fig = plt.figure(figsize=(12,5), dpi=100)\n",
    "    plt.subplot(1,2,1)\n",
    "    samples = samples.view(100, *img_size)\n",
    "    file_name = os.path.join(out_dir, '%08d.png' % iteration)\n",
    "    vision_utils.save_image(samples, file_name, nrow=10)\n",
    "    grid_img = vision_utils.make_grid(samples, nrow=10, normalize=True, padding=0)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0), interpolation='nearest')\n",
    "    plt.subplot(1,2,2)\n",
    "    metrics = get_metrics(pretrained_clf, num_samples_eval, mu_real, sigma_real, G)\n",
    "    fids.append(metrics['fid'])\n",
    "    inception_means.append(metrics['inception_mean'])\n",
    "    inception_stds.append(metrics['inception_std'])\n",
    "    if G_avg is not None:\n",
    "      metrics = get_metrics(pretrained_clf, num_samples_eval, mu_real, sigma_real, G_avg)\n",
    "      fids_avg.append(metrics['fid'])\n",
    "      inception_means_avg.append(metrics['inception_mean'])\n",
    "    if G_ema is not None:\n",
    "      metrics = get_metrics(pretrained_clf, num_samples_eval, mu_real, sigma_real, G_ema)\n",
    "      fids_ema.append(metrics['fid'])\n",
    "      inception_means_ema.append(metrics['inception_mean'])\n",
    "    iterations.append(iteration)\n",
    "    times.append(time_tick)\n",
    "    #  is\n",
    "    is_low  = [m - s for m, s in zip(inception_means, inception_stds)]\n",
    "    is_high = [m + s for m, s in zip(inception_means, inception_stds)]\n",
    "    plt.plot(times, inception_means, label=\"is\", color='r')\n",
    "    plt.fill_between(times, is_low, is_high, facecolor='r', alpha=.3)\n",
    "    plt.yticks(np.arange(0, 10+1, 0.5))\n",
    "    # fid\n",
    "    plt.plot(times, fids, label=\"fid\", color='b')\n",
    "    plt.xlabel('Time (sec)')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.grid()\n",
    "    ax = fig.gca()\n",
    "    ax.set_ylim(-0.1, 10)\n",
    "    plt.legend(fancybox=True, framealpha=.5)\n",
    "    curves_img_file_name = os.path.join(out_dir, 'curves.png')\n",
    "    fig.savefig(curves_img_file_name)\n",
    "    plt.show()\n",
    "    curves_file_name = os.path.join(out_dir, 'curves.json')\n",
    "    curves = {\n",
    "        'inception_means': list(inception_means),\n",
    "        'inception_stds': list(inception_stds),\n",
    "        'inception_means_ema': list(inception_means_ema),\n",
    "        'inception_means_avg': list(inception_means_avg),\n",
    "        'fids_ema': list(fids_ema),\n",
    "        'fids_avg': list(fids_avg),\n",
    "        'fids': list(fids),\n",
    "        'iterations':iterations,\n",
    "        'times': times\n",
    "    }\n",
    "    with open(curves_file_name, 'w') as fs:\n",
    "      json.dump(curves, fs)\n",
    "  return plot_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wh8ag1jR9t2K"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "huCcsTk6Q3Nr"
   },
   "source": [
    "### AltGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xL2nfwtcP_YH"
   },
   "outputs": [],
   "source": [
    "args = dict(iterations = 100000,\n",
    "            batch_size = 50,\n",
    "            lrD = 0.001,\n",
    "            lrG = 0.001,\n",
    "            beta1 = 0.05,\n",
    "            extragrad = False,\n",
    "            eval_every = 1000,\n",
    "            lookahead = False,\n",
    "            eval_avg = True,\n",
    "            lookahead_k = 1000,\n",
    "            n_workers = 5,\n",
    "            device = 'cuda',\n",
    "            grad_max_norm = None)\n",
    "\n",
    "\n",
    "for k in range(1,5+1):\n",
    "  exp_key = f\"iter{args['iterations']}_bs{args['batch_size']}_lrD{args['lrD']}\" + \\\n",
    "            f\"_lrG{args['lrG']}_beta1{args['beta1']}_lookahead{args['lookahead']}\" + \\\n",
    "            f\"_lak{args['lookahead_k']}\" + \\\n",
    "            f\"_extragrad{args['extragrad']}_ee{args['eval_every']}\"\n",
    "  out_dir = f\"./drive/My Drive/results/final/{exp_key}/{k}/\"\n",
    "\n",
    "  shutil.rmtree(out_dir, ignore_errors=True)\n",
    "  if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "  with open(os.path.join(out_dir, 'args.json'), 'w') as fs:\n",
    "    json.dump(args, fs)\n",
    "\n",
    "  dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "\n",
    "  plot_func = get_plot_func(out_dir=out_dir, \n",
    "                            img_size=dataset[0][0].size(),\n",
    "                            num_samples_eval=10000)\n",
    "\n",
    "  G = GeneratorCNN28(noise_dim=_NOISE_DIM, out_tanh=True)\n",
    "  D = DiscriminatorCNN28(spectral_norm=False, img_size=28)\n",
    "\n",
    "  train(G, D, dataset, \n",
    "        iterations=args['iterations'], \n",
    "        batch_size=args['batch_size'], \n",
    "        lookahead=args['lookahead'],\n",
    "        lookahead_k=args['lookahead_k'],\n",
    "        eval_avg=args['eval_avg'],\n",
    "        lrD=args['lrD'], \n",
    "        lrG=args['lrG'], \n",
    "        beta1=args['beta1'], \n",
    "        extragrad=args['extragrad'],\n",
    "        eval_every=args['eval_every'], \n",
    "        n_workers=args['n_workers'], \n",
    "        device=torch.device(args['device']), \n",
    "        grad_max_norm=args['grad_max_norm'], \n",
    "        plot_func=plot_func,\n",
    "        out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZqWS0kTSnQ1"
   },
   "source": [
    "### LA-AltGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkuSgAcWSnax"
   },
   "outputs": [],
   "source": [
    "args = dict(iterations = 100000,\n",
    "            batch_size = 50,\n",
    "            lrD = 0.001,\n",
    "            lrG = 0.001,\n",
    "            beta1 = 0.05,\n",
    "            extragrad = False,\n",
    "            eval_every = 1000,\n",
    "            lookahead = True,\n",
    "            eval_avg = True,\n",
    "            lookahead_k = 1000,\n",
    "            n_workers = 5,\n",
    "            device = 'cuda',\n",
    "            grad_max_norm = None)\n",
    "\n",
    "\n",
    "for k in range(1,5+1):\n",
    "  exp_key = f\"iter{args['iterations']}_bs{args['batch_size']}_lrD{args['lrD']}\" + \\\n",
    "            f\"_lrG{args['lrG']}_beta1{args['beta1']}_lookahead{args['lookahead']}\" + \\\n",
    "            f\"_lak{args['lookahead_k']}\" + \\\n",
    "            f\"_extragrad{args['extragrad']}_ee{args['eval_every']}\"\n",
    "  out_dir = f\"./drive/My Drive/results/final/{exp_key}/{k}/\"\n",
    "\n",
    "  shutil.rmtree(out_dir, ignore_errors=True)\n",
    "  if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "  with open(os.path.join(out_dir, 'args.json'), 'w') as fs:\n",
    "    json.dump(args, fs)\n",
    "\n",
    "  dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "\n",
    "  plot_func = get_plot_func(out_dir=out_dir, \n",
    "                            img_size=dataset[0][0].size(),\n",
    "                            num_samples_eval=10000)\n",
    "\n",
    "  G = GeneratorCNN28(noise_dim=_NOISE_DIM, out_tanh=True)\n",
    "  D = DiscriminatorCNN28(spectral_norm=False, img_size=28)\n",
    "\n",
    "  train(G, D, dataset, \n",
    "        iterations=args['iterations'], \n",
    "        batch_size=args['batch_size'], \n",
    "        lookahead=args['lookahead'],\n",
    "        lookahead_k=args['lookahead_k'],\n",
    "        eval_avg=args['eval_avg'],\n",
    "        lrD=args['lrD'], \n",
    "        lrG=args['lrG'], \n",
    "        beta1=args['beta1'], \n",
    "        extragrad=args['extragrad'],\n",
    "        eval_every=args['eval_every'], \n",
    "        n_workers=args['n_workers'], \n",
    "        device=torch.device(args['device']), \n",
    "        grad_max_norm=args['grad_max_norm'], \n",
    "        plot_func=plot_func,\n",
    "        out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m75vdvWvThjt"
   },
   "source": [
    "### ExtraGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jrf8CZ5VThp6"
   },
   "outputs": [],
   "source": [
    "args = dict(iterations = 100000,\n",
    "            batch_size = 50,\n",
    "            lrD = 0.001,\n",
    "            lrG = 0.001,\n",
    "            beta1 = 0.05,\n",
    "            extragrad = True,\n",
    "            eval_every = 1000,\n",
    "            lookahead = False,\n",
    "            eval_avg = True,\n",
    "            lookahead_k = 1000,\n",
    "            n_workers = 5,\n",
    "            device = 'cuda',\n",
    "            grad_max_norm = None)\n",
    "\n",
    "\n",
    "for k in range(1,5+1):\n",
    "  exp_key = f\"iter{args['iterations']}_bs{args['batch_size']}_lrD{args['lrD']}\" + \\\n",
    "            f\"_lrG{args['lrG']}_beta1{args['beta1']}_lookahead{args['lookahead']}\" + \\\n",
    "            f\"_lak{args['lookahead_k']}\" + \\\n",
    "            f\"_extragrad{args['extragrad']}_ee{args['eval_every']}\"\n",
    "  out_dir = f\"./drive/My Drive/results/final-joint/{exp_key}/{k}/\"\n",
    "\n",
    "  shutil.rmtree(out_dir, ignore_errors=True)\n",
    "  if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "  with open(os.path.join(out_dir, 'args.json'), 'w') as fs:\n",
    "    json.dump(args, fs)\n",
    "\n",
    "  dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "\n",
    "  plot_func = get_plot_func(out_dir=out_dir, \n",
    "                            img_size=dataset[0][0].size(),\n",
    "                            num_samples_eval=10000)\n",
    "\n",
    "  G = GeneratorCNN28(noise_dim=_NOISE_DIM, out_tanh=True)\n",
    "  D = DiscriminatorCNN28(spectral_norm=False, img_size=28)\n",
    "\n",
    "  train(G, D, dataset, \n",
    "        iterations=args['iterations'], \n",
    "        batch_size=args['batch_size'], \n",
    "        lookahead=args['lookahead'],\n",
    "        lookahead_k=args['lookahead_k'],\n",
    "        eval_avg=args['eval_avg'],\n",
    "        lrD=args['lrD'], \n",
    "        lrG=args['lrG'], \n",
    "        beta1=args['beta1'], \n",
    "        extragrad=args['extragrad'],\n",
    "        eval_every=args['eval_every'], \n",
    "        n_workers=args['n_workers'], \n",
    "        device=torch.device(args['device']), \n",
    "        grad_max_norm=args['grad_max_norm'], \n",
    "        plot_func=plot_func,\n",
    "        out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqOvCEXoTsp5"
   },
   "source": [
    "### LA-ExtraGrad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CcxhEVUxTsxP"
   },
   "outputs": [],
   "source": [
    "args = dict(iterations = 100000,\n",
    "            batch_size = 50,\n",
    "            lrD = 0.001,\n",
    "            lrG = 0.001,\n",
    "            beta1 = 0.05,\n",
    "            extragrad = True,\n",
    "            eval_every = 1000,\n",
    "            lookahead = True,\n",
    "            eval_avg = True,\n",
    "            lookahead_k = 1000,\n",
    "            n_workers = 5,\n",
    "            device = 'cuda',\n",
    "            grad_max_norm = None)\n",
    "\n",
    "\n",
    "for k in range(1,5+1):\n",
    "  exp_key = f\"iter{args['iterations']}_bs{args['batch_size']}_lrD{args['lrD']}\" + \\\n",
    "            f\"_lrG{args['lrG']}_beta1{args['beta1']}_lookahead{args['lookahead']}\" + \\\n",
    "            f\"_lak{args['lookahead_k']}\" + \\\n",
    "            f\"_extragrad{args['extragrad']}_ee{args['eval_every']}\"\n",
    "  out_dir = f\"./drive/My Drive/results/final/{exp_key}/{k}/\"\n",
    "\n",
    "  shutil.rmtree(out_dir, ignore_errors=True)\n",
    "  if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "  with open(os.path.join(out_dir, 'args.json'), 'w') as fs:\n",
    "    json.dump(args, fs)\n",
    "\n",
    "  dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "\n",
    "  plot_func = get_plot_func(out_dir=out_dir, \n",
    "                            img_size=dataset[0][0].size(),\n",
    "                            num_samples_eval=10000)\n",
    "\n",
    "  G = GeneratorCNN28(noise_dim=_NOISE_DIM, out_tanh=True)\n",
    "  D = DiscriminatorCNN28(spectral_norm=False, img_size=28)\n",
    "\n",
    "  train(G, D, dataset, \n",
    "        iterations=args['iterations'], \n",
    "        batch_size=args['batch_size'], \n",
    "        lookahead=args['lookahead'],\n",
    "        lookahead_k=args['lookahead_k'],\n",
    "        eval_avg=args['eval_avg'],\n",
    "        lrD=args['lrD'], \n",
    "        lrG=args['lrG'], \n",
    "        beta1=args['beta1'], \n",
    "        extragrad=args['extragrad'],\n",
    "        eval_every=args['eval_every'], \n",
    "        n_workers=args['n_workers'], \n",
    "        device=torch.device(args['device']), \n",
    "        grad_max_norm=args['grad_max_norm'], \n",
    "        plot_func=plot_func,\n",
    "        out_dir=out_dir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "LA-AltGAN+LA-ExtraGrad.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
