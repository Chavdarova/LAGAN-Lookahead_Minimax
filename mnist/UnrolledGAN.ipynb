{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQ1cgc7Y_0bH"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "le9R65tBOEiW",
    "outputId": "21c21f52-3f3c-4b22-db9f-4f8bc90df861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting higher\n",
      "  Downloading https://files.pythonhosted.org/packages/84/bb/56ef38a860a3ad56e1a517042ceb03aacf0e5e071c976a3311775de0a1c8/higher-0.2-py3-none-any.whl\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from higher) (1.5.0+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->higher) (1.18.4)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->higher) (0.16.0)\n",
      "Installing collected packages: higher\n",
      "Successfully installed higher-0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gGThtfYc_lKU"
   },
   "outputs": [],
   "source": [
    "import higher\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import torchvision.utils as vision_utils\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.distributions import bernoulli\n",
    "from scipy import linalg\n",
    "import torchvision.datasets as _datasets\n",
    "import torchvision.transforms as _transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "pqWHhTnIAbKD",
    "outputId": "5000b649-2075-42dc-f5ae-cd4717f49d52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  2 00:28:10 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "KltKnDnTw8sM",
    "outputId": "e4a882bc-acda-4ebb-985c-50481e969b2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7rmPgOn_4Yk"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "suSPfCc-_0vc"
   },
   "outputs": [],
   "source": [
    "_NOISE_DIM = 128\n",
    "_H_FILTERS = 64\n",
    "\n",
    "\n",
    "class DiscriminatorCNN28(nn.Module):\n",
    "\n",
    "    def __init__(self, img_channels=1, h_filters=_H_FILTERS,\n",
    "                 spectral_norm=False, img_size=None, n_outputs=1):\n",
    "        if any(not isinstance(_arg, int) for _arg in [img_channels, h_filters, n_outputs]):\n",
    "            raise TypeError(\"Unsupported operand type. Expected integer.\")\n",
    "        if not isinstance(spectral_norm, bool):\n",
    "            raise TypeError(f\"Unsupported operand type: {type(spectral_norm)}. \"\n",
    "                            \"Expected bool.\")\n",
    "        if min([img_channels, h_filters, n_outputs]) <= 0:\n",
    "            raise ValueError(\"Expected nonzero positive input arguments for: the \"\n",
    "                             \"number of output channels, the dimension of the noise \"\n",
    "                             \"vector, as well as the depth of the convolution kernels.\")\n",
    "        super(DiscriminatorCNN28, self).__init__()\n",
    "        # _conv = nn.utils.spectral_norm(nn.Conv2d) if spectral_norm else nn.Conv2d\n",
    "        _apply_sn = lambda x: nn.utils.spectral_norm(x) if spectral_norm else x\n",
    "        self.img_channels = img_channels\n",
    "        self.img_size = img_size\n",
    "        self.n_outputs = n_outputs\n",
    "        self.main = nn.Sequential(\n",
    "            _apply_sn(nn.Conv2d(img_channels, h_filters, 4, 2, 1, bias=False)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            _apply_sn(nn.Conv2d(h_filters, h_filters * 2, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(h_filters * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            _apply_sn(nn.Conv2d(h_filters * 2, h_filters * 4, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(h_filters * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            _apply_sn(nn.Conv2d(h_filters * 4, self.n_outputs, 3, 1, 0, bias=False))        \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.img_channels is not None and self.img_size is not None:\n",
    "            if numpy.prod(list(x.size())) % (self.img_size ** 2 * self.img_channels) != 0:\n",
    "                raise ValueError(f\"Size mismatch. Input size: {numpy.prod(list(x.size()))}. \"\n",
    "                                 f\"Expected input divisible by: {self.noise_dim}\")\n",
    "            x = x.view(-1, self.img_channels, self.img_size, self.img_size)\n",
    "        x = self.main(x)\n",
    "        return x.view(-1, self.n_outputs)\n",
    "\n",
    "    def load(self, model):\n",
    "      self.load_state_dict(model.state_dict())\n",
    "\n",
    "\n",
    "class GeneratorCNN28(nn.Module):\n",
    "\n",
    "    def __init__(self, img_channels=1, noise_dim=_NOISE_DIM, h_filters=_H_FILTERS, out_tanh=False):\n",
    "        if any(not isinstance(_arg, int) for _arg in [img_channels, noise_dim, h_filters]):\n",
    "            raise TypeError(\"Unsupported operand type. Expected integer.\")\n",
    "        if min([img_channels, noise_dim, h_filters]) <= 0:\n",
    "            raise ValueError(\"Expected strictly positive input arguments for the \"\n",
    "                             \"number of output channels, the dimension of the noise \"\n",
    "                             \"vector, as well as the depth of the convolution kernels.\")\n",
    "        super(GeneratorCNN28, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(noise_dim, h_filters * 8, 3, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(_H_FILTERS * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(h_filters * 8, h_filters * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(_H_FILTERS * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(h_filters * 4, h_filters * 2, 4, 2, 0, bias=False),\n",
    "            nn.BatchNorm2d(_H_FILTERS * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(h_filters * 2, img_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh() if out_tanh else nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if numpy.prod(list(x.size())) % self.noise_dim != 0:\n",
    "            raise ValueError(f\"Size mismatch. Input size: {numpy.prod(list(x.size()))}. \"\n",
    "                             f\"Expected input divisible by: {self.noise_dim}\")\n",
    "        x = x.view(-1, self.noise_dim, 1, 1)\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "\n",
    "    def load(self, model):\n",
    "      self.load_state_dict(model.state_dict())\n",
    "\n",
    "\n",
    "class MLP_mnist(nn.Module):\n",
    "  def __init__(self, input_dims, n_hiddens, n_class):\n",
    "    super(MLP_mnist, self).__init__()\n",
    "    assert isinstance(input_dims, int), 'Expected int for input_dims'\n",
    "    self.input_dims = input_dims\n",
    "    current_dims = input_dims\n",
    "    layers = OrderedDict()\n",
    "\n",
    "    if isinstance(n_hiddens, int):\n",
    "      n_hiddens = [n_hiddens]\n",
    "    else:\n",
    "      n_hiddens = list(n_hiddens)\n",
    "    for i, n_hidden in enumerate(n_hiddens):\n",
    "      layers['fc{}'.format(i+1)] = nn.Linear(current_dims, n_hidden)\n",
    "      layers['relu{}'.format(i+1)] = nn.ReLU()\n",
    "      layers['drop{}'.format(i+1)] = nn.Dropout(0.2)\n",
    "      current_dims = n_hidden\n",
    "    layers['out'] = nn.Linear(current_dims, n_class)\n",
    "    self.layers = layers\n",
    "    self.model= nn.Sequential(layers)\n",
    "    #print(self.model)\n",
    "\n",
    "  def forward(self, input):\n",
    "    input = input.view(input.size(0), -1)\n",
    "    assert input.size(1) == self.input_dims\n",
    "    return self.model.forward(input)\n",
    "\n",
    "  def get_logits_and_fc2_outputs(self, x):\n",
    "    x = x.view(x.size(0), -1)\n",
    "    assert x.size(1) == self.input_dims\n",
    "    fc2_out = None\n",
    "    for l in self.model:\n",
    "      x = l(x)\n",
    "      if l == self.layers[\"fc2\"]:\n",
    "        fc2_out = x\n",
    "    return x, fc2_out\n",
    "\n",
    "\n",
    "def pretrained_mnist_model(input_dims=784, n_hiddens=[256, 256], n_class=10, \n",
    "                           pretrained=None):\n",
    "    model = MLP_mnist(input_dims, n_hiddens, n_class)\n",
    "    if pretrained is not None:\n",
    "        if os.path.exists(pretrained):\n",
    "            print('Loading trained model from %s' % pretrained)\n",
    "            state_dict = torch.load(pretrained,\n",
    "                    map_location='cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "            if 'parallel' in pretrained:\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in state_dict.items():\n",
    "                    name = k[7:]  # remove `module.`\n",
    "                    new_state_dict[name] = v\n",
    "                state_dict = new_state_dict\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Could not find pretrained model: {pretrained}.\")\n",
    "        model.load_state_dict(state_dict)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDYOE4e8DRAY"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylrrJcSnDQOP"
   },
   "outputs": [],
   "source": [
    "class Binarize(object):\n",
    "  def __init__(self, threshold=0.3):\n",
    "    self.threshold = threshold\n",
    "      \n",
    "  def __call__(self, t):\n",
    "    t = (t > self.threshold).float()\n",
    "    return t\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return self.__class__.__name__ + '(th={0})'.format(self.threshold)\n",
    "\n",
    "\n",
    "class Smooth(object):\n",
    "  def __init__(self, smooth=0.1):\n",
    "    self.smooth = smooth\n",
    "      \n",
    "  def __call__(self, t):\n",
    "    t[t == 1.] = 1 - self.smooth\n",
    "    t[t == 0.] = 0 + self.smooth\n",
    "    return t\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return self.__class__.__name__ + '(smooth={0})'.format(self.smooth)\n",
    "\n",
    "\n",
    "def load_mnist(_data_root='datasets', binarized=False, bin_th=0.3, smooth=None):\n",
    "    trans = [_transforms.ToTensor()]\n",
    "    if binarized:\n",
    "      binarizor = Binarize(bin_th)\n",
    "      trans.append(binarizor)\n",
    "    if smooth is not None:\n",
    "      smoother = Smooth(smooth)\n",
    "      trans.append(smoother)\n",
    "    trans = _transforms.Compose(trans)\n",
    "    _data = _datasets.MNIST(_data_root, train=True, download=True,\n",
    "                            transform=trans)\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y-YEPGTyKwYZ"
   },
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XCCAWH7lKwkv"
   },
   "outputs": [],
   "source": [
    "def get_disciminator_loss(D, x_real, x_gen, lbl_real, lbl_fake):\n",
    "  \"\"\"\"\"\"\n",
    "  D_x = D(x_real)\n",
    "  D_G_z = D(x_gen)\n",
    "  lossD_real = torch.binary_cross_entropy_with_logits(D_x, lbl_real).mean()\n",
    "  lossD_fake = torch.binary_cross_entropy_with_logits(D_G_z, lbl_fake).mean()\n",
    "  lossD = lossD_real + lossD_fake\n",
    "  return lossD\n",
    "\n",
    "\n",
    "def get_generator_loss(G, D, z, lbl_real):\n",
    "  \"\"\"\"\"\"\n",
    "  D_G_z = D(G(z))\n",
    "  lossG = torch.binary_cross_entropy_with_logits(D_G_z, lbl_real).mean()\n",
    "  return lossG\n",
    "\n",
    "\n",
    "def get_sampler(dataset, batch_size, shuffle=True, num_workers=1, drop_last=True):\n",
    "  dataloader = DataLoader(dataset, batch_size, shuffle=shuffle, \n",
    "                          num_workers=num_workers, drop_last=drop_last)\n",
    "  dataloader_iterator = iter(dataloader)\n",
    "  def sampler():\n",
    "    nonlocal dataloader_iterator\n",
    "    try:\n",
    "        data = next(dataloader_iterator) \n",
    "    except StopIteration:\n",
    "        dataloader_iterator = iter(dataloader)\n",
    "        data = next(dataloader_iterator) \n",
    "    return data\n",
    "  return sampler\n",
    "\n",
    "\n",
    "def update_avg_gen(G, G_avg, n_gen_update):\n",
    "    \"\"\" Updates the uniform average generator. \"\"\"\n",
    "    l_param = list(G.parameters())\n",
    "    l_avg_param = list(G_avg.parameters())\n",
    "    if len(l_param) != len(l_avg_param):\n",
    "        raise ValueError(\"Got different lengths: {}, {}\".format(len(l_param), len(l_avg_param)))\n",
    "\n",
    "    for i in range(len(l_param)):\n",
    "        with torch.no_grad():\n",
    "            l_avg_param[i].data.copy_(l_avg_param[i].data.mul(n_gen_update).div(n_gen_update + 1.).add(\n",
    "                                      l_param[i].data.div(n_gen_update + 1.)))\n",
    "\n",
    "def update_ema_gen(G, G_ema, beta_ema=0.9999):\n",
    "    \"\"\" Updates the exponential moving average generator. \"\"\"\n",
    "    l_param = list(G.parameters())\n",
    "    l_ema_param = list(G_ema.parameters())\n",
    "    if len(l_param) != len(l_ema_param):\n",
    "        raise ValueError(\"Got different lengths: {}, {}\".format(len(l_param), len(l_ema_param)))\n",
    "\n",
    "    for i in range(len(l_param)):\n",
    "        with torch.no_grad():\n",
    "            l_ema_param[i].data.copy_(l_ema_param[i].data.mul(beta_ema).add(\n",
    "                l_param[i].data.mul(1-beta_ema)))\n",
    "\n",
    "\n",
    "def train_old(G, D, dataset, iterations, batch_size=32, lrD=0.01, lrG=0.01, \n",
    "          beta1=0.99, eval_every=100, n_workers=5, device=torch.device('cpu'), \n",
    "          grad_max_norm=1, plot_func=lambda a,b,c,d: None, unroll_D=0, \n",
    "          unroll_G=0, eval_avg=False):\n",
    "  \n",
    "  sampler = get_sampler(dataset, batch_size, shuffle=True, \n",
    "                        num_workers=n_workers, drop_last=True)\n",
    "\n",
    "  # Optimizers\n",
    "  optimizerD = torch.optim.Adam(D.parameters(), lr=lrD, betas=(beta1, 0.999))\n",
    "  optimizerG = torch.optim.Adam(G.parameters(), lr=lrG, betas=(beta1, 0.999))\n",
    "\n",
    "  # LBLs\n",
    "  lbl_real = torch.ones( batch_size, 1, device=device)\n",
    "  lbl_fake = torch.zeros(batch_size, 1, device=device)\n",
    "\n",
    "  fixed_noise = torch.randn(100, G.noise_dim, device=device)\n",
    "\n",
    "  G.to(device)\n",
    "  D.to(device)\n",
    "\n",
    "  G_avg, G_ema = None, None\n",
    "  if eval_avg:\n",
    "    G_avg = copy.deepcopy(G)\n",
    "    G_ema = copy.deepcopy(G)\n",
    "\n",
    "  start_time = time.perf_counter()\n",
    "\n",
    "  for i in range(iterations):\n",
    "    x_real, _ = sampler()\n",
    "\n",
    "    # STEP 1: Unroll G\n",
    "    if unroll_G > 0:\n",
    "      backup_G = copy.deepcopy(G)\n",
    "      for _ in range(unroll_G):\n",
    "        optimizerG.zero_grad()\n",
    "        z = torch.randn(batch_size, G.noise_dim, device=device)\n",
    "        lossG = get_generator_loss(G, D, z, lbl_real)\n",
    "        lossG.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "    # STEP 2: D optimization step\n",
    "    x_real = x_real.to(device)\n",
    "    z = torch.randn(batch_size, G.noise_dim, device=device)\n",
    "    with torch.no_grad():\n",
    "      x_gen = G(z) # using the unrolled G\n",
    "    optimizerD.zero_grad()\n",
    "    lossD = get_disciminator_loss(D, x_real, x_gen, lbl_real, lbl_fake)\n",
    "    lossD.backward()\n",
    "    if grad_max_norm is not None:\n",
    "      nn.utils.clip_grad_norm_(D.parameters(), grad_max_norm)\n",
    "    optimizerD.step()\n",
    "\n",
    "    if unroll_G > 0: # We restore the original G\n",
    "      G.load(backup_G)\n",
    "      del backup_G\n",
    "\n",
    "    # STEP 3: Unroll D (from the original D, not the new one)\n",
    "    if unroll_D > 0:\n",
    "      final_D = copy.deepcopy(D)\n",
    "      for _ in range(unroll_D):\n",
    "        optimizerD.zero_grad()\n",
    "        x_real, _ = sampler()\n",
    "        x_real = x_real.to(device)\n",
    "        z = torch.randn(batch_size, G.noise_dim, device=device)\n",
    "        with torch.no_grad():\n",
    "          x_gen = G(z)\n",
    "        lossD = get_disciminator_loss(D, x_real, x_gen, lbl_real, lbl_fake)\n",
    "        lossD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "    # STEP 4: G optimization step\n",
    "    z = torch.randn(batch_size, G.noise_dim, device=device)\n",
    "    optimizerG.zero_grad()\n",
    "    lossG = get_generator_loss(G, D, z, lbl_real) # we use the unrolled D\n",
    "    lossG.backward()\n",
    "    if grad_max_norm is not None:\n",
    "      nn.utils.clip_grad_norm_(G.parameters(), grad_max_norm)\n",
    "    optimizerG.step()\n",
    "\n",
    "    if unroll_D > 0:\n",
    "      D.load(final_D)\n",
    "      del final_D\n",
    "\n",
    "    if eval_avg:\n",
    "      update_avg_gen(G, G_avg, i)\n",
    "      update_ema_gen(G, G_ema, beta_ema=0.9999)\n",
    "\n",
    "    # Just plotting things\n",
    "    if i % eval_every == 0 or i == iterations-1:\n",
    "      with torch.no_grad():\n",
    "        probas = torch.sigmoid(D(G(fixed_noise)))\n",
    "        mean_proba = probas.mean().cpu().item()\n",
    "        std_proba = probas.std().cpu().item()\n",
    "        samples = G(fixed_noise)\n",
    "      print(f\"Iter {i}: Mean proba from D(G(z)): {mean_proba:.4f} +/- {std_proba:.4f}\")\n",
    "      plot_func(samples.detach().cpu(), time_tick=time.perf_counter() - start_time, D=D, G=G, iteration=i, G_avg=G_avg, G_ema=G_ema)\n",
    "  \n",
    "\n",
    "def train(G, D, dataset, iterations, batch_size=32, lrD=0.01, lrG=0.01, \n",
    "          beta1=0.99, eval_every=100, n_workers=5, device=torch.device('cpu'), \n",
    "          grad_max_norm=1, plot_func=lambda a,b,c,d: None, \n",
    "          unroll_D=0, eval_avg=False):\n",
    "  \n",
    "  sampler = get_sampler(dataset, batch_size, shuffle=True, \n",
    "                        num_workers=n_workers, drop_last=True)\n",
    "\n",
    "  # Optimizers\n",
    "  optimizerD = torch.optim.Adam(D.parameters(), lr=lrD, betas=(beta1, 0.999))\n",
    "  optimizerG = torch.optim.Adam(G.parameters(), lr=lrG, betas=(beta1, 0.999))\n",
    "\n",
    "  # LBLs\n",
    "  lbl_real = torch.ones( batch_size, 1, device=device)\n",
    "  lbl_fake = torch.zeros(batch_size, 1, device=device)\n",
    "\n",
    "  fixed_noise = torch.randn(100, G.noise_dim, device=device)\n",
    "\n",
    "  G.to(device)\n",
    "  D.to(device)\n",
    "\n",
    "  G_avg, G_ema = None, None\n",
    "  if eval_avg:\n",
    "    G_avg = copy.deepcopy(G)\n",
    "    G_ema = copy.deepcopy(G)\n",
    "\n",
    "  start_time = time.perf_counter()\n",
    "\n",
    "  for i in range(iterations):\n",
    "\n",
    "    # STEP 1: D optimization step\n",
    "    x_real, _ = sampler()\n",
    "    x_real = x_real.to(device)\n",
    "    z = torch.randn(batch_size, G.noise_dim, device=device)\n",
    "    with torch.no_grad():\n",
    "      x_gen = G(z)\n",
    "    optimizerD.zero_grad()\n",
    "    lossD = get_disciminator_loss(D, x_real, x_gen, lbl_real, lbl_fake)\n",
    "    lossD.backward()\n",
    "    if grad_max_norm is not None:\n",
    "      nn.utils.clip_grad_norm_(D.parameters(), grad_max_norm)\n",
    "    optimizerD.step()\n",
    "\n",
    "    # STEP 2: Unroll G using an urolled D over unroll_D steps\n",
    "    if unroll_D > 0:\n",
    "      optimizerD.zero_grad()\n",
    "      with higher.innerloop_ctx(D, optimizerD) as (unrolled_D, diffopt):\n",
    "        for _ in range(unroll_D):\n",
    "          z = torch.randn(batch_size, G.noise_dim, device=device)\n",
    "          x_real, _ = sampler()\n",
    "          x_real = x_real.to(device)\n",
    "          with torch.no_grad():\n",
    "            x_gen = G(z)\n",
    "          lossD = get_disciminator_loss(unrolled_D, x_real, x_gen, lbl_real, lbl_fake)\n",
    "          diffopt.step(lossD) # Taking a differentiable step\n",
    "        # STEP 2': G optimization step\n",
    "        z = torch.randn(batch_size, G.noise_dim, device=device)\n",
    "        optimizerG.zero_grad()\n",
    "        lossG = get_generator_loss(G, unrolled_D, z, lbl_real) # we use the unrolled D\n",
    "        lossG.backward()\n",
    "        if grad_max_norm is not None:\n",
    "          nn.utils.clip_grad_norm_(G.parameters(), grad_max_norm)\n",
    "        optimizerG.step()\n",
    "    else:\n",
    "      z = torch.randn(batch_size, G.noise_dim, device=device)\n",
    "      optimizerG.zero_grad()\n",
    "      lossG = get_generator_loss(G, D, z, lbl_real) # we use the non-unrolled D\n",
    "      lossG.backward()\n",
    "      if grad_max_norm is not None:\n",
    "        nn.utils.clip_grad_norm_(G.parameters(), grad_max_norm)\n",
    "      optimizerG.step()\n",
    "\n",
    "    if eval_avg:\n",
    "      update_avg_gen(G, G_avg, i)\n",
    "      update_ema_gen(G, G_ema, beta_ema=0.9999)\n",
    "\n",
    "    # Just plotting things\n",
    "    if i % eval_every == 0 or i == iterations-1:\n",
    "      with torch.no_grad():\n",
    "        probas = torch.sigmoid(D(G(fixed_noise)))\n",
    "        mean_proba = probas.mean().cpu().item()\n",
    "        std_proba = probas.std().cpu().item()\n",
    "        samples = G(fixed_noise)\n",
    "      print(f\"Iter {i}: Mean proba from D(G(z)): {mean_proba:.4f} +/- {std_proba:.4f}\")\n",
    "      plot_func(samples.detach().cpu(), time_tick=time.perf_counter() - start_time, D=D, G=G, iteration=i, G_avg=G_avg, G_ema=G_ema)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JEMA5J179bCN"
   },
   "source": [
    "# Display & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tBw8aDhlAMx"
   },
   "outputs": [],
   "source": [
    "def compute_mu_sigma_pretrained_model(dataset, pretrained_clf):\n",
    "  dataloader = DataLoader(dataset, batch_size=512, num_workers=2, drop_last=True)\n",
    "  cuda = next(pretrained_clf.parameters()).is_cuda\n",
    "  all_fc2_out = []\n",
    "  pretrained_clf.eval()\n",
    "  for batch, _ in dataloader:\n",
    "    with torch.no_grad():\n",
    "      if cuda:\n",
    "        batch = batch.cuda()\n",
    "      _, fc2_out = pretrained_clf.get_logits_and_fc2_outputs(batch)\n",
    "    all_fc2_out.append(fc2_out.cpu())\n",
    "  all_fc2_out = torch.cat(all_fc2_out, dim=0).numpy()\n",
    "  mu_real = np.mean(all_fc2_out, axis=0)\n",
    "  sigma_real = np.cov(all_fc2_out, rowvar=False)\n",
    "  return mu_real, sigma_real\n",
    "\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\"\"\"\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n",
    "    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    # product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n",
    "        print(msg)\n",
    "        # warnings.warn(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    # numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(\"Imaginary component {}\".format(m))\n",
    "        covmean = covmean.real\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "\n",
    "\n",
    "def _calculate_metrics(pretrained_clf, G, dataset_length, mu_real, sigma_real, \n",
    "                       n_classes=10, batch_size=1024):\n",
    "    cuda = next(pretrained_clf.parameters()).is_cuda\n",
    "    if cuda:\n",
    "      device = torch.device('cuda')\n",
    "    else:\n",
    "      device = torch.device('cpu')\n",
    "    # Using pretrained clf to get predictions over fake data\n",
    "    inception_predictions, all_fc2_out, class_probas = [], [], []\n",
    "    dataloader = DataLoader(list(range(dataset_length)), batch_size, num_workers=2, drop_last=True)\n",
    "    pretrained_clf.eval()\n",
    "    for batch in dataloader:\n",
    "      with torch.no_grad():\n",
    "        noise = torch.randn(batch_size, G.noise_dim, device=device)\n",
    "        probas, fc2_out = pretrained_clf.get_logits_and_fc2_outputs(G(noise).view(batch_size, -1))\n",
    "      all_fc2_out.append(fc2_out.cpu())\n",
    "      class_probas.append(probas.cpu())\n",
    "    all_fc2_out = torch.cat(all_fc2_out, dim=0).numpy()\n",
    "    class_probas = torch.cat(class_probas, dim=0)\n",
    "    inception_predictions = torch.softmax(class_probas, dim=1).numpy()\n",
    "    class_probas = class_probas.numpy()\n",
    "    pred_prob = np.maximum(class_probas, 1e-20 * np.ones_like(class_probas))\n",
    "\n",
    "    y_vec = 1e-20 * np.ones((len(pred_prob), n_classes), dtype=np.float)  # pred label distr\n",
    "    gnd_vec = 0.1 * np.ones((1, n_classes), dtype=np.float)  # gnd label distr, uniform over classes\n",
    "\n",
    "    for i, label in enumerate(pred_prob):\n",
    "        y_vec[i, np.argmax(pred_prob[i])] = 1.0\n",
    "    y_vec = np.sum(y_vec, axis=0, keepdims=True)\n",
    "    y_vec = y_vec / np.sum(y_vec)\n",
    "\n",
    "    label_entropy = np.sum(-y_vec * np.log(y_vec)).tolist()\n",
    "    label_tv = np.true_divide(np.sum(np.abs(y_vec - gnd_vec)), 2).tolist()\n",
    "    label_l2 = np.sum((y_vec - gnd_vec) ** 2).tolist()\n",
    "\n",
    "    # --- is ----\n",
    "    inception_scores = []\n",
    "    for i in range(n_classes):\n",
    "        part = inception_predictions[(i * inception_predictions.shape[0]\n",
    "                                      // n_classes):((i + 1) * inception_predictions.shape[0]\n",
    "                                                     // n_classes), :]\n",
    "        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
    "        kl = np.mean(np.sum(kl, 1))\n",
    "        inception_scores.append(np.exp(kl))\n",
    "\n",
    "    mu = np.mean(all_fc2_out, axis=0)\n",
    "    sigma = np.cov(all_fc2_out, rowvar=False)\n",
    "    _fid = calculate_frechet_distance(mu, sigma, mu_real, sigma_real)\n",
    "\n",
    "    return (label_entropy, label_tv, label_l2,\n",
    "            float(np.mean(inception_scores)),\n",
    "            float(np.std(inception_scores)),\n",
    "            _fid)\n",
    "\n",
    "\n",
    "def get_metrics(pretrained_clf, dataset_length, mu_real, sigma_real, G):\n",
    "    \"\"\"Calculates entropy, TV, L2, and inception scores.\"\"\"\n",
    "    e, tv, l2, is_m, is_std, fid = _calculate_metrics(pretrained_clf,\n",
    "                                                      G,\n",
    "                                                      dataset_length,\n",
    "                                                      mu_real,\n",
    "                                                      sigma_real)\n",
    "    m_result = {\n",
    "        'entropy': e,\n",
    "        'TV': tv,\n",
    "        'L2': l2,\n",
    "        'inception_mean': is_m,\n",
    "        'inception_std': is_std,\n",
    "        'fid': fid\n",
    "    }\n",
    "    return m_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185,
     "referenced_widgets": [
      "0c4bc835b1214bccbb43287442f7e7ba",
      "13c4753df3924ee3830d87d1bdc31f08",
      "3076df64e9124553b1fb3e40329b87b2",
      "1f6e3266f7914a60924ea10e2fa1bdb5",
      "1b623195613b4883a68364b913b2f578",
      "37c5120cd4e34af4950ccc98aaf48920",
      "2caef99f68aa41a5b106ca43bb7681b4",
      "c7aaa3b949c04974a06489d03967186d"
     ]
    },
    "colab_type": "code",
    "id": "Kakyriotn0hX",
    "outputId": "5389a1fc-7db8-4eff-f636-b055f61e42d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"http://ml.cs.tsinghua.edu.cn/~chenxi/pytorch-models/mnist-b07bb66b.pth\" to /root/.cache/torch/checkpoints/mnist-b07bb66b.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4bc835b1214bccbb43287442f7e7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1078198.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'L2': 0.886607586895978,\n",
       " 'TV': 0.8932725694444446,\n",
       " 'entropy': 0.048469967731797015,\n",
       " 'fid': 23.97065921781831,\n",
       " 'inception_mean': 1.0859612226486206,\n",
       " 'inception_std': 0.002975340001285076}"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title small test\n",
    "pretrained_clf = pretrained_mnist_model(pretrained='./pretrained_models/mnist.pth')\n",
    "G = GeneratorCNN28(noise_dim=_NOISE_DIM)\n",
    "G.to(torch.device('cuda'))\n",
    "dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "\n",
    "mu_real, sigma_real = compute_mu_sigma_pretrained_model(dataset, pretrained_clf)\n",
    "\n",
    "metrics = get_metrics(pretrained_clf, 10000, mu_real, sigma_real, G)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9T2JfCAd5SXA"
   },
   "outputs": [],
   "source": [
    "def get_plot_func(out_dir, img_size, num_samples_eval=10000, save_curves=None):\n",
    "  dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "  #shutil.rmtree(out_dir, ignore_errors=True)\n",
    "  #if not os.path.exists(out_dir):\n",
    "  #  os.makedirs(out_dir)\n",
    "  pretrained_clf = pretrained_mnist_model(pretrained='./drive/My Drive/Data/models/mnist.pth')\n",
    "  mu_real, sigma_real = compute_mu_sigma_pretrained_model(dataset, pretrained_clf)\n",
    "  inception_means, inception_stds, inception_means_ema, inception_means_avg, fids, fids_ema, fids_avg = [], [], [], [], [], [], []\n",
    "  iterations, times = [], []\n",
    "  def plot_func(samples, iteration, time_tick, G=None, D=None, G_avg=None, G_ema=None):\n",
    "    fig = plt.figure(figsize=(12,5), dpi=100)\n",
    "    plt.subplot(1,2,1)\n",
    "    samples = samples.view(100, *img_size)\n",
    "    file_name = os.path.join(out_dir, '%08d.png' % iteration)\n",
    "    vision_utils.save_image(samples, file_name, nrow=10)\n",
    "    grid_img = vision_utils.make_grid(samples, nrow=10, normalize=True, padding=0)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0), interpolation='nearest')\n",
    "    plt.subplot(1,2,2)\n",
    "    metrics = get_metrics(pretrained_clf, num_samples_eval, mu_real, sigma_real, G)\n",
    "    fids.append(metrics['fid'])\n",
    "    inception_means.append(metrics['inception_mean'])\n",
    "    inception_stds.append(metrics['inception_std'])\n",
    "    if G_avg is not None:\n",
    "      metrics = get_metrics(pretrained_clf, num_samples_eval, mu_real, sigma_real, G_avg)\n",
    "      fids_avg.append(metrics['fid'])\n",
    "      inception_means_avg.append(metrics['inception_mean'])\n",
    "    if G_ema is not None:\n",
    "      metrics = get_metrics(pretrained_clf, num_samples_eval, mu_real, sigma_real, G_ema)\n",
    "      fids_ema.append(metrics['fid'])\n",
    "      inception_means_ema.append(metrics['inception_mean'])\n",
    "    iterations.append(iteration)\n",
    "    times.append(time_tick)\n",
    "    #  is\n",
    "    is_low  = [m - s for m, s in zip(inception_means, inception_stds)]\n",
    "    is_high = [m + s for m, s in zip(inception_means, inception_stds)]\n",
    "    plt.plot(times, inception_means, label=\"is\", color='r')\n",
    "    plt.fill_between(times, is_low, is_high, facecolor='r', alpha=.3)\n",
    "    plt.yticks(np.arange(0, 10+1, 0.5))\n",
    "    # fid\n",
    "    plt.plot(times, fids, label=\"fid\", color='b')\n",
    "    plt.xlabel('Time (sec)')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.grid()\n",
    "    ax = fig.gca()\n",
    "    ax.set_ylim(-0.1, 10)\n",
    "    plt.legend(fancybox=True, framealpha=.5)\n",
    "    curves_img_file_name = os.path.join(out_dir, 'curves.png')\n",
    "    fig.savefig(curves_img_file_name)\n",
    "    plt.show()\n",
    "    curves_file_name = os.path.join(out_dir, 'curves.json')\n",
    "    curves = {\n",
    "        'inception_means': list(inception_means),\n",
    "        'inception_stds': list(inception_stds),\n",
    "        'inception_means_ema': list(inception_means_ema),\n",
    "        'inception_means_avg': list(inception_means_avg),\n",
    "        'fids_ema': list(fids_ema),\n",
    "        'fids_avg': list(fids_avg),\n",
    "        'fids': list(fids),\n",
    "        'iterations':iterations,\n",
    "        'times': times\n",
    "    }\n",
    "    with open(curves_file_name, 'w') as fs:\n",
    "      json.dump(curves, fs)\n",
    "  return plot_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wh8ag1jR9t2K"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xjW3JZhvD58f"
   },
   "source": [
    "#### Approx unrolling D on 20 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5UirXZUD2aG"
   },
   "outputs": [],
   "source": [
    "args = dict(iterations = 100000,\n",
    "            batch_size = 50,\n",
    "            lrD = 0.001,\n",
    "            lrG = 0.001,\n",
    "            beta1 = 0.05,\n",
    "            unroll_D = 20,\n",
    "            eval_every = 1000,\n",
    "            n_workers = 5,\n",
    "            device = 'cuda',\n",
    "            grad_max_norm = None)\n",
    "\n",
    "\n",
    "for k in range(1,5+1):\n",
    "  exp_key = f\"iter{args['iterations']}_bs{args['batch_size']}_lrD{args['lrD']}\" + \\\n",
    "            f\"_lrG{args['lrG']}_beta1{args['beta1']}\" + \\\n",
    "            f\"_ApproxUrD{args['unroll_D']}_ee{args['eval_every']}\"\n",
    "  out_dir = f\"./drive/My Drive/results/final/{exp_key}/{k}/\"\n",
    "\n",
    "  shutil.rmtree(out_dir, ignore_errors=True)\n",
    "  if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "  with open(os.path.join(out_dir, 'args.json'), 'w') as fs:\n",
    "    json.dump(args, fs)\n",
    "\n",
    "  dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "\n",
    "  plot_func = get_plot_func(out_dir=out_dir, \n",
    "                            img_size=dataset[0][0].size(),\n",
    "                            num_samples_eval=10000)\n",
    "\n",
    "  G = GeneratorCNN28(noise_dim=_NOISE_DIM, out_tanh=True)\n",
    "  D = DiscriminatorCNN28(spectral_norm=False, img_size=28)\n",
    "\n",
    "  train_old(G, D, dataset, \n",
    "        iterations=args['iterations'], \n",
    "        batch_size=args['batch_size'], \n",
    "        lrD=args['lrD'], \n",
    "        lrG=args['lrG'], \n",
    "        beta1=args['beta1'], \n",
    "        unroll_D=args['unroll_D'],\n",
    "        eval_every=args['eval_every'], \n",
    "        n_workers=args['n_workers'], \n",
    "        device=torch.device(args['device']), \n",
    "        grad_max_norm=args['grad_max_norm'], \n",
    "        plot_func=plot_func,\n",
    "        eval_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9EAlZ3GdGDUf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "UnrolledGAN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c4bc835b1214bccbb43287442f7e7ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3076df64e9124553b1fb3e40329b87b2",
       "IPY_MODEL_1f6e3266f7914a60924ea10e2fa1bdb5"
      ],
      "layout": "IPY_MODEL_13c4753df3924ee3830d87d1bdc31f08"
     }
    },
    "13c4753df3924ee3830d87d1bdc31f08": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b623195613b4883a68364b913b2f578": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "1f6e3266f7914a60924ea10e2fa1bdb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7aaa3b949c04974a06489d03967186d",
      "placeholder": "​",
      "style": "IPY_MODEL_2caef99f68aa41a5b106ca43bb7681b4",
      "value": " 1.03M/1.03M [00:11&lt;00:00, 96.2kB/s]"
     }
    },
    "2caef99f68aa41a5b106ca43bb7681b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3076df64e9124553b1fb3e40329b87b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37c5120cd4e34af4950ccc98aaf48920",
      "max": 1078198,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1b623195613b4883a68364b913b2f578",
      "value": 1078198
     }
    },
    "37c5120cd4e34af4950ccc98aaf48920": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7aaa3b949c04974a06489d03967186d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
